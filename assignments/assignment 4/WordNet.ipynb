{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Part 0 - Basic Setup"
      ],
      "metadata": {
        "id": "DB1_Cf50Elym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('book')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Yq6HkhMl0oFh",
        "outputId": "64e66c21-8df7-4902-cdca-c046992be594"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1 - Summary of WordNet"
      ],
      "metadata": {
        "id": "Scr99X5Lo-HZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNet is a large database that groups cognitive synonyms of verbs, nouns, adjectives, and adverbs into sets called synsets. These synsets are then linked with other synsets using lexical relations to form a network of synsets."
      ],
      "metadata": {
        "id": "SBcINc8dpFrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2"
      ],
      "metadata": {
        "id": "zQGn5meS0VJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select a noun. Output all synsets.**"
      ],
      "metadata": {
        "id": "ZgHEN7iY0juv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "noun = 'star'\n",
        "synsets = wn.synsets(noun)\n",
        "print(synsets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PHliwLb801b8",
        "outputId": "e2f23e57-1644-4a1f-dd6c-39ef1c3e4c3a"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('star.n.01'), Synset('ace.n.03'), Synset('star.n.03'), Synset('star.n.04'), Synset('star.n.05'), Synset('headliner.n.01'), Synset('asterisk.n.01'), Synset('star_topology.n.01'), Synset('star.v.01'), Synset('star.v.02'), Synset('star.v.03'), Synset('leading.s.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3"
      ],
      "metadata": {
        "id": "cWlQB2_N2m5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select one synset from the list of synsets. Extract its definition, usage examples, and lemmas.\n",
        "From your selected synset, traverse up the WordNet hierarchy as far as you can, outputting the\n",
        "synsets as you go. Write a couple of sentences observing the way that WordNet is organized for\n",
        "nouns.**"
      ],
      "metadata": {
        "id": "AJ6OLwB72oPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synset = synsets[0]\n",
        "\n",
        "print(synset.definition())\n",
        "print(synset.examples())\n",
        "print(synset.lemmas())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Qa18KMxZ2tFX",
        "outputId": "8e441f48-2412-4551-d265-08976adbbb6e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(astronomy) a celestial body of hot gases that radiates energy derived from thermonuclear reactions in the interior\n",
            "[]\n",
            "[Lemma('star.n.01.star')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyper = synset.hypernyms()[0]\n",
        "while hyper:\n",
        "  print(hyper)\n",
        "  hyper = hyper.hypernyms()\n",
        "  if hyper:\n",
        "    hyper = hyper[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mwJIBnIb4fL7",
        "outputId": "dea9d299-8173-433c-dbb2-d92f08de5087"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('celestial_body.n.01')\n",
            "Synset('natural_object.n.01')\n",
            "Synset('whole.n.02')\n",
            "Synset('object.n.01')\n",
            "Synset('physical_entity.n.01')\n",
            "Synset('entity.n.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I noticed that for nouns WordNet is organized in a hierarchical manner. At the very top levels all nouns are entities, and a star is a physical object. Perhaps a non-physical noun would be something like anger."
      ],
      "metadata": {
        "id": "G2FRJpBR6BT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4"
      ],
      "metadata": {
        "id": "uPbiAw8S6ZT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output the following (or an empty list if none exist): hypernyms, hyponyms, meronyms,\n",
        "holonyms, antonym.**"
      ],
      "metadata": {
        "id": "mPzr9VQM6fs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(synset.hypernyms())\n",
        "print(synset.hyponyms())\n",
        "print(synset.part_meronyms())\n",
        "print(synset.part_holonyms())\n",
        "print(synset.lemmas()[0].antonyms())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rYfVeadC6lq2",
        "outputId": "4fcc3900-47bb-4e68-f173-7232b10db5b1"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('celestial_body.n.01')]\n",
            "[Synset('binary_star.n.01'), Synset('fixed_star.n.01'), Synset('giant_star.n.01'), Synset('lodestar.n.01'), Synset('multiple_star.n.01'), Synset('neutron_star.n.01'), Synset('nova.n.01'), Synset('red_dwarf.n.01'), Synset('red_giant.n.01'), Synset('sun.n.04'), Synset('supergiant.n.01'), Synset('supernova.n.01'), Synset('variable_star.n.01'), Synset('white_dwarf.n.01')]\n",
            "[]\n",
            "[]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 5"
      ],
      "metadata": {
        "id": "HLOmSZ_v7ZAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select a verb. Output all synsets.**"
      ],
      "metadata": {
        "id": "jmkMtkd37adH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verb = 'condense'\n",
        "v_synsets = wn.synsets(verb)\n",
        "print(v_synsets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vB-pXZpn7doF",
        "outputId": "af06c4fd-0ca4-4ea5-82bc-9ef9c041350f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('condense.v.01'), Synset('digest.v.07'), Synset('condense.v.03'), Synset('condense.v.04'), Synset('condense.v.05'), Synset('condense.v.06'), Synset('condense.v.07')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 6"
      ],
      "metadata": {
        "id": "O9e7H5Ll7qpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select one synset from the list of synsets. Extract its definition, usage examples, and lemmas.\n",
        "From your selected synset, traverse up the WordNet hierarchy as far as you can, outputting the\n",
        "synsets as you go. Write a couple of sentences observing the way that WordNet is organized for\n",
        "verbs.**"
      ],
      "metadata": {
        "id": "ijvrDfu_7r4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v_synset = v_synsets[0]\n",
        "print(v_synset.definition())\n",
        "print(v_synset.examples())\n",
        "print(v_synset.lemmas())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TXH3EnRd7wEC",
        "outputId": "0e6d8db1-e4af-4f11-b32d-c5deea2192a3"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "undergo condensation; change from a gaseous to a liquid state and fall in drops\n",
            "['water condenses', 'The acid distills at a specific temperature']\n",
            "[Lemma('condense.v.01.condense'), Lemma('condense.v.01.distill'), Lemma('condense.v.01.distil')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyper = v_synset.hypernyms()[0]\n",
        "while hyper:\n",
        "  print(hyper)\n",
        "  hyper = hyper.hypernyms()\n",
        "  if hyper:\n",
        "    hyper = hyper[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZSD2qdps8EF9",
        "outputId": "37f8fa5c-d51d-435d-9139-d13b6e320475"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('liquefy.v.03')\n",
            "Synset('change_integrity.v.01')\n",
            "Synset('change.v.02')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I noticed that there is not top level analogue for 'entity' (for noun synsets) in verb synsets. As such, since verbs are harder to organize there seem to be fewer hierarchical levels."
      ],
      "metadata": {
        "id": "09BQao4j8LHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 7"
      ],
      "metadata": {
        "id": "dHvt9QjP8aL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use morphy to find as many different forms of the word as you can.**"
      ],
      "metadata": {
        "id": "o_g0YLOD8bmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wn.morphy('condensing'))\n",
        "print(wn.morphy('condensed'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qWwci5TB8fIO",
        "outputId": "1f7827e2-ea75-4a9b-8a16-8b955bd65a81"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "condensing\n",
            "condense\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 8"
      ],
      "metadata": {
        "id": "YTGo5rpt9cOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select two words that you think might be similar. Find the specific synsets you are interested in.\n",
        "Run the Wu-Palmer similarity metric and the Lesk algorithm. Write a couple of sentences with\n",
        "your observations.**"
      ],
      "metadata": {
        "id": "NascO7c09das"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.wsd import lesk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "word1, word2 = 'photon.n.01', 'electron.n.01'\n",
        "syn1, syn2 = wn.synset(word1), wn.synset(word2)\n",
        "print('Wu-Palmer:', wn.wup_similarity(syn1, syn2))\n",
        "\n",
        "sent = 'A photon was used in the dual-slit experiment, uncovering the basics of quantum mechanics.'\n",
        "tokens = word_tokenize(sent)\n",
        "print(lesk(tokens, 'photon'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NYgkvSLB9hJZ",
        "outputId": "dd3b81a7-f7f6-4aca-db40-cd3f8c54c70e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wu-Palmer: 0.7\n",
            "Synset('photon.n.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose photon and electron because both are very small subatomic particles that exhibit quantum mechanical effects. I think that a score of 0.7 is pretty realistic, given that the particles are still different despite sharing a similar behavior. The lesk algorithm returned the correct synset for photon, given that photon.n.01 describes the photon as the quantum (smallest particle) of the electromagnetic field."
      ],
      "metadata": {
        "id": "r_EDohWHAulk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 9"
      ],
      "metadata": {
        "id": "nctwlNE3BqEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write a couple of sentences about SentiWordNet, describing its functionality and possible use\n",
        "cases. Select an emotionally charged word. Find its senti-synsets and output the polarity scores\n",
        "for each word. Make up a sentence. Output the polarity for each word in the sentence. Write a\n",
        "couple of sentences about your observations of the scores and the utility of knowing these\n",
        "scores in an NLP application.**"
      ],
      "metadata": {
        "id": "Cd3-l13RBrPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SentiWordNet is a library built on top of WordNet that can assign 3 sentiment values to a given synset: positive, negative, and objectivity. These 3 values always add up to one."
      ],
      "metadata": {
        "id": "52ZWPxagB0eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import sentiwordnet as swn\n",
        "\n",
        "word = 'irate'\n",
        "\n",
        "senti_sets = swn.senti_synsets(word)\n",
        "all = list(senti_sets)\n",
        "print(all)\n",
        "\n",
        "irate = all[0]\n",
        "\n",
        "print('Positive score =', irate.pos_score())\n",
        "print('Negative score =', irate.neg_score())\n",
        "print('Objective score =', irate.obj_score())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mpFBDO4HEKJD",
        "outputId": "cc0a3d3e-3322-4511-be11-b2d523197077"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SentiSynset('irate.s.01')]\n",
            "Positive score = 0.125\n",
            "Negative score = 0.5\n",
            "Objective score = 0.375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'I hate those who are irate.'\n",
        "words = [word for word in word_tokenize(sent) if word.isalpha()]\n",
        "for word in words:\n",
        "  all = list(swn.senti_synsets(word))\n",
        "  print('\\n' + word + ' scores:')\n",
        "  if all:\n",
        "    sentiment = all[0]\n",
        "  else:\n",
        "    print(None)\n",
        "    continue\n",
        "  print('Positive score =', sentiment.pos_score())\n",
        "  print('Negative score =', sentiment.neg_score())\n",
        "  print('Objective score =', sentiment.obj_score())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "p9_iQ4ZBG1Vt",
        "outputId": "ac022de7-e1dc-4a36-8948-8f5bbafda4be"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I scores:\n",
            "Positive score = 0.0\n",
            "Negative score = 0.0\n",
            "Objective score = 1.0\n",
            "\n",
            "hate scores:\n",
            "Positive score = 0.125\n",
            "Negative score = 0.375\n",
            "Objective score = 0.5\n",
            "\n",
            "those scores:\n",
            "None\n",
            "\n",
            "who scores:\n",
            "Positive score = 0.0\n",
            "Negative score = 0.0\n",
            "Objective score = 1.0\n",
            "\n",
            "are scores:\n",
            "Positive score = 0.0\n",
            "Negative score = 0.0\n",
            "Objective score = 1.0\n",
            "\n",
            "irate scores:\n",
            "Positive score = 0.125\n",
            "Negative score = 0.5\n",
            "Objective score = 0.375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seeing all of the sentiment scores for these synsets, it seems very easy to naively score words that carry little sentiment, like 'I' or 'are.' However, when it comes to emotion carrying words (actual sentiment), the sentiment scores don't necessarily fit the contexts that they are used in. In this particular case, hate should probably be more negative while irate should be more objective."
      ],
      "metadata": {
        "id": "XOcyzPJnITYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 10"
      ],
      "metadata": {
        "id": "TxJ7YuzoI9vl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write a couple of sentences about what a collocation is. Output collocations for text4, the\n",
        "Inaugural corpus. Select one of the collocations identified by NLTK. Calculate mutual\n",
        "information. Write commentary on the results of the mutual information formula and your\n",
        "interpretation.**"
      ],
      "metadata": {
        "id": "H8vuLemFI-65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A collocation is a combination of words that frequently or conventionally appear together. An example is 'pay attention'; another could be 'dark day.'"
      ],
      "metadata": {
        "id": "iVFWVY3vJCnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import *\n",
        "import math\n",
        "\n",
        "text = ' '.join(text4.tokens)\n",
        "\n",
        "my_collocations = text4.collocation_list()\n",
        "print(my_collocations)\n",
        "\n",
        "selected = my_collocations[1]\n",
        "# selected = ['it', 'is']\n",
        "\n",
        "string = ' '.join(selected)\n",
        "num_tokens = len(text4.tokens)\n",
        "\n",
        "num_joint = text.count(string)\n",
        "num_x = text.count(selected[0])\n",
        "num_y = text.count(selected[1])\n",
        "\n",
        "\n",
        "pxy = num_joint / (num_tokens - 1)\n",
        "px = num_x / num_tokens\n",
        "py = num_y / num_tokens\n",
        "\n",
        "print(pxy, px, py)\n",
        "\n",
        "pmi = math.log2(pxy / (px * py))\n",
        "print(pmi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xIkLYqYIbgaY",
        "outputId": "4a8b8429-7f5f-4ac6-c68c-5e954b774ade"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('United', 'States'), ('fellow', 'citizens'), ('years', 'ago'), ('four', 'years'), ('Federal', 'Government'), ('General', 'Government'), ('American', 'people'), ('Vice', 'President'), ('God', 'bless'), ('Chief', 'Justice'), ('one', 'another'), ('fellow', 'Americans'), ('Old', 'World'), ('Almighty', 'God'), ('Fellow', 'citizens'), ('Chief', 'Magistrate'), ('every', 'citizen'), ('Indian', 'tribes'), ('public', 'debt'), ('foreign', 'nations')]\n",
            "0.001079136690647482 0.04638295367590794 0.033865049934271196\n",
            "-0.541586104386413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of the formula for mutual information makes sense. Essentially if a phrase has a higher probability of appearing together than the probability that the words are independent from each other, then we can conclude that the word is a collocation. As such, even words that often appear together (like 'it' and 'is') are not collocations because they appear independently very frequently."
      ],
      "metadata": {
        "id": "2Nnh33yvvZsU"
      }
    }
  ]
}